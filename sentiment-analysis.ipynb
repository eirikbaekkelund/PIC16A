{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## Group Names and Roles\n",
    "\n",
    "- Partner 1 (Role)\n",
    "- Partner 2 (Role)\n",
    "- Partner 3 (Role)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this problem, you will study the sentiment in a data set of tweets collected during the COVID-19 pandemic. To load the data set, run this block: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def grab_tweets():\n",
    "    \"\"\"\n",
    "    The user supplied these data already split into training and test sets. \n",
    "    This function downloads and combines them, returning a single data frame.\n",
    "    No arguments. \n",
    "    \"\"\"\n",
    "\n",
    "    url1 = \"https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/datasets/Corona_NLP_train.csv\"\n",
    "    url2 = \"https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/datasets/Corona_NLP_test.csv\"\n",
    "    \n",
    "    df1 = pd.read_csv(url1, encoding='iso-8859-1')\n",
    "    df2 = pd.read_csv(url2, encoding='iso-8859-1')\n",
    "    \n",
    "    return pd.concat((df1, df2), axis = 0).reset_index().drop(\"index\", axis = 1)\n",
    "\n",
    "df = grab_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §1.Take a Look \n",
    "\n",
    "Briefly inspect the data `df`. The `OriginalTweet` column contains the text of the tweet, and the `Sentiment` column contains a text label describing the sentiment of the tweet as described by a crowdsourced worker. \n",
    "\n",
    "Create a `pandas` summary table showing how many tweets there are for each sentiment. \n",
    "\n",
    "**Hint**: `groupby().size()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OriginalTweet                                                                                                                                                                                                                                                Sentiment         \n",
       "      Coronavirus lingers in air longer than previously thought scientists warn                                                                                                                                                                              Negative              1\n",
       "      amp                                                                                                                                                                                                                                                    Neutral               1\n",
       "    Hand Sanitizer Free Gift For Current Situation                                                                                                                                                                                                           Extremely Positive    1\n",
       "    Our medical frontliners are our country s first line of defense in our fight against COVID 19 As our way of showing our appreciation they are given access to the priority lanes at Robinsons Supermarket                                                Positive              1\n",
       "    Police officers handed out rolls of toilet paper at a supermarket on Thursday to try to calm shoppers down during the outbreak in                                                                                                                        Positive              1\n",
       "                                                                                                                                                                                                                                                                                  ..\n",
       "Â«Â Industrial real-estate operators expect the disruption of consumer supply chains caused by the coronavirus pandemic to drive a new surge in #warehousing demandÂ Â» #RealEstate #COVID2019 #logistics\\r\\r\\nhttps://t.co/0jiC0w0yGZ                       Negative              1\n",
       "Â«Â Well, officer, it is quite simple actually: I cycled up the hill to the supermarket to buy catÂs food, so I ticked all thÃ© boxesÂ Â». #MaVieConfinee #confinementjour2 #COVID2019 #relax ?? https://t.co/WYRIReGhen                                    Negative              1\n",
       "Â» CONSUMER ALERT: Coronavirus (COVID-19): Know Your Rights | Attorney General Karl A. Racine https://t.co/5pJGz2aRNk                                                                                                                                        Positive              1\n",
       "Ãa se peut?  Why Does Covid-19 Make Some People So Sick? Ask Their DNA Consumer genomics company 23andMe wants to mine its database of millions of customers for clues to why the virus hits some https://t.co/7uPouMm12j                                   Extremely Negative    1\n",
       "Ã  As buyers stock up, exports of cereals, processed food, spices spike\\r\\r\\n\\r\\r\\nÃ  South-based auto makers suspend production in fight against Covid-19\\r\\r\\n\\r\\r\\nÃ  With April 1 deadline looming, more than 7 lakh BS-IV vehicles are lying unsold  Extremely Negative    1\n",
       "Length: 44955, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.groupby(['OriginalTweet', 'Sentiment']).size()\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §2. Create the Term-Document Matrix\n",
    "\n",
    "Now, use the `CountVectorizer` from `sklearn.feature_extraction.text` to construct a term-document matrix. You should add the columns of this matrix to the original `df`, resulting in a single data frame that contains both the word counts and the sentiments. \n",
    "\n",
    "I constructed my `CountVectorizer` with these settings:\n",
    "\n",
    "```CountVectorizer(max_df = 0.2, min_df = 50, stop_words = 'english')```\n",
    "\n",
    "However, you are free to experiment with different ones if you'd like to. You may find it useful to consult code from the [lecture on the term-document matrix](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_1.ipynb) or the [lecture on sentiment analysis](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_3.ipynb) for this part.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §3.Train-Test Split\n",
    "\n",
    "Split the data into training and test sets. Use 50% of the data for training and 50% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §4.Prepare Predictor and Target Variables\n",
    "\n",
    "So far, we've split our data into training and test sets, but we also need to separate out the predictor and target variables. We should also make sure that both sets of variables are encoded as numeric columns that a machine learning algorithm can understand. \n",
    "\n",
    "Write a function called `prepare_variables` to perform these tasks. It should have two return values: \n",
    "\n",
    "- `X`, the predictor columns. I suggest dropping all columns except the ones that you constructed as part of the term-document matrix. You can do this using `df.drop(list_of_columns, axis = 1)`. \n",
    "- `y`, the target column. There are multiple valid ways to construct `y` from the original `Sentiment` column of the data. For this activity, `y[i]` should be equal to `1` if `df[\"Sentiment\"][i]` contains the word `\"Positive\"` and `0` otherwise. \n",
    "    - ***Hint***: `df[\"column\"].str.contains(\"word\")`\n",
    "\n",
    "After you've defined your function, use it to prepare the training and test sets:  \n",
    "\n",
    "```\n",
    "X_train, y_train = prepare_variables(train) \n",
    "X_test,  y_test  = prepare_variables(test)\n",
    "```\n",
    "\n",
    "Check these sets to ensure that your function does what you'd expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §5. Fit the model\n",
    "\n",
    "Fit a logistic regression model to the training data. It is possible to perform cross-validation to set model complexity parameters, but we won't worry about that today. \n",
    "\n",
    "You may receive a warning indicating that \"TOTAL NO. of ITERATIONS REACHED LIMIT.\" This is a little bit intimidating, but in this case it's not really harmful. It's fine to ignore this warning, or you can set the parameter `max_iter` when you construct the `LogisticRegression()` model to a higher value, such as `500` or `1000`. This may increase the amount of time required to fit your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, evaluate the accuracy of your model on the training and testing data. You may observe a small amount of overfitting, indicated by the testing score being lower than the training score. Provided that both are over 80%, you're doing fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §6. Positive and Negative Words\n",
    "\n",
    "The *coefficients* associated by logistic regression to each word are stored in the `LR.coef_[0]` attribute. Show lists of the most positive and most negative words. Discuss your findings -- do these words look as though they would reasonably convey positive or negative sentiment?\n",
    "\n",
    "***Hint***: *There are several good ways to do this. I found that the easiest way was to create a new `Dataframe` containing `LR.coef_[0]` in one column and `X_train.columns` (the list of words) in the other column. I then used `df.sort_values()` to sort on the column containing coefficients. The [lecture on sentiment analysis](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_3.ipynb) illustrates this method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §7. Inspect Errors\n",
    "\n",
    "Finally, we should take a look at a few examples in which the model made a mistake on the test data. Inspect three tweets in which the model's prediction and the actual value differ. Comment on why you think the error may have occurred. \n",
    "\n",
    "**Don't overthink it!** It's sufficient to simply identify three rows in which the prediction disagrees with the true label. You shouldn't need more than 3-4 lines. \n",
    "\n",
    "***Note***: *One valid response is to disagree with the sentiment label assigned to the tweet -- some of them are a little odd.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
